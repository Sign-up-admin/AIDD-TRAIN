# 数据模块 (`compass/data`)

## 详细模块文档

*   [核心思想与工作流程](./CORE_IDEAS.md)
*   [**数据模块 (本文档)**](./DATA_MODULE.md)
*   [模型模块 (`compass/training/model.py`)](./MODEL_MODULE.md)
*   [引擎模块 (`compass/engine`)](./ENGINE_MODULE.md)
*   [优化器模块 (`scripts/hardware_optimizer.py`)](./OPTIMIZER.md)

---

## 1. 核心解决的矛盾

想象一个场景：你是一位计算化学研究员，刚有了一个关于新型图神经网络的绝妙想法，用于预测蛋白质-配体的结合亲和力。你手头有标准的 PDBbind 数据集，里面是成千上万的 PDB 和 SDF 文件。

你面临的核心矛盾是：**你的原始数据（基于文本的原子坐标文件）与你的[模型模块](./MODEL_MODULE.md)真正需要的输入（结构化的数值图对象）之间存在巨大的鸿沟。**

本模块的使命就是高效、可靠地跨越这一鸿沟。它要解决以下几个具体且棘手的工程问题：

1.  **从文本到张量 (Feature Engineering)**: 如何将抽象的化学概念（如原子符号 'C'、'N'、'O'）和物理信息（三维坐标）转化为神经网络能够理解和学习的数值特征向量？
2.  **定义图结构 (Graph Construction)**: 如何将配体和蛋白质的原子组合成一个统一的图？如何定义原子之间的连接（边）？如何确保两者合并时原子不会重叠？
3.  **等待的痛苦 (Efficiency)**: PDBbind 数据集非常庞大。如果串行处理数万个文件，可能需要数小时，这会严重扼杀研究的迭代速度。如何利用多核 CPU 并行处理，将等待时间从数小时缩短到几分钟？
4.  **避免重复工作 (Caching)**: 数据处理是计算密集型操作。每次运行实验都从头处理一遍数据是无法接受的。如何建立一个智能的缓存机制，既能重用已处理好的数据，又能在你修改了处理逻辑后，知道需要强制重新生成？

[数据模块 (`compass/data`)](./DATA_MODULE.md) 就是为了解决这些问题而设计的，它是一个健壮、可并行、带缓存的流水线，能将原始化学文件无缝、高效地转换为模型就绪的图数据。

## 2. 工作流程与核心思想

本模块的核心思想是 **“原始数据 -> 并行处理 -> 缓存化图数据”** 的流水线。

1.  **入口 (`dataset.py`)**: `PDBBindDataset` 类是整个数据流水线的总指挥。当它被实例化时，会检查 `processed_data` 目录下是否已经存在处理好的数据。
    - 如果存在且用户未在 `config.py` 中强制要求重新处理 (`force_data_reprocessing: false`)，则直接加载缓存文件，极大提高了启动速度。
    - 如果不存在或用户要求强制刷新，它将启动 `process()` 方法。

2.  **并行调度 (`dataset.py`)**: `process()` 方法不会单打独斗，它会将每一个待处理的 PDB 条目打包成一个独立的“任务”，然后利用 Python 的 `multiprocessing.Pool` 将这些任务分发给多个 CPU核心并行执行。这使得数据处理速度能随 CPU 核心数近似线性提升。

3.  **原子化处理 (`processing.py`)**: `process_item` 函数是每个并行任务的核心执行单元，它负责对 **单个** PDB 条目进行精细化处理：
    - **加载与过滤**: 使用 RDKit 加载配体和蛋白质分子。同时进行“健康检查”，如文件大小、原子数量是否超出 `config.py` 中预设的阈值 (`max_atoms`)，过滤掉异常数据。
    - **图的构建 (`graph/conversion.py`)**: 将配体和蛋白质的分子结构分别转换为图。这里的核心是 **避免原子位置重叠**：在处理蛋白质时，会排除掉那些已经属于配体的原子坐标，确保最终图的纯净性。
    - **特征生成 (`graph/features.py`)**: 为图中的每个原子（节点）生成特征向量。例如，将'C', 'O', 'N' 等元素符号转换为 one-hot 编码。
    - **数据组装**: 将配体图和蛋白质图合并为一个统一的 `torch_geometric.data.Data` 对象，并将结合亲和力数据 (`y`) 作为图的标签。
    - **最终校验**: 在返回之前，会进行最后一次检查，去除因浮点数精度问题可能导致的坐标完全相同的重复原子，确保数据质量。

4.  **输出**: `process_item` 处理完成的 `Data` 对象会被 `torch.save` 保存到 `processed_data` 目录下的对应位置，以 `.pt` 文件格式作为缓存。

## 3. 常见问题与解决方案 (FAQ)

- **问：数据处理非常缓慢，如何提速？**
  - **答：** 在 `config.py` 中，增大 `num_workers` 的值。建议设置为您机器 CPU 核心数或稍小的值，以最大化并行处理效率。

- **问：我修改了特征提取的逻辑（例如 `features.py`），但再次运行时模型用的还是旧数据？**
  - **答：** 这是因为缓存机制。为了让您的修改生效，请在 `config.py` 中设置 `force_data_reprocessing: true`。这会删除旧的 `processed_data` 目录，并强制重新生成所有数据。

- **问：日志中出现大量“Skipping PDB...”的警告，是什么原因？**
  - **答：** 请检查 `logs/processing.log` 文件获取详细原因。常见原因包括：
    1.  蛋白质文件过大或原子数过多（超过 `config.py` 中的 `max_atoms` 配置）。
    2.  RDKit 无法解析分子结构，可能是原始 PDB 文件格式不标准。
    3.  配体文件（.sdf 或 .mol2）损坏或无法加载。

- **问：日志中出现“removed duplicate atom(s)”的警告，严重吗？**
  - **答：** 通常不严重。这是系统在合并配体和蛋白质时的一个最终安全检查，用于处理因坐标精度问题导致的微小重叠。只要不是所有数据都出现此警告，可以认为是正常现象。
