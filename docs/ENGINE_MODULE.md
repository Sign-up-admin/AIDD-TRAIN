# 引擎模块 (`compass/engine`)

## 1. 核心解决的矛盾

训练一个复杂的深度学习模型，远不止是写一个 `for` 循环那么简单。本引擎模块的核心使命是解决训练过程中的三大主要矛盾：

1.  **稳定性与效率的矛盾**：如何在利用 GPU 加速（如混合精度）的同时，保证训练过程的数值稳定性（如梯度裁剪、学习率动态调整）？
2.  **健壮性与开发体验的矛盾**：如何在训练意外中断（如断电、用户`Ctrl+C`）后，能无缝恢复训练，避免数小时乃至数天的计算成果付诸东流？
3.  **逻辑与模板代码的分离**：如何将核心的“前向-反向-优化”逻辑与复杂的“检查点管理、信号处理、日志记录”等模板代码解耦，保持代码的清晰与可维护性？

## 2. 工作流程与核心思想

本模块采用 **“指挥官-执行者” (Conductor-Executor)** 的设计模式，将复杂的训练流程拆分为两个层次：

-   **`Trainer` 类 (`trainer.py`) - 指挥官**：负责顶层调度和状态管理，不关心每个批次的具体计算。
-   **`train/validate_epoch` 函数 (`loop.py`) - 执行者**：负责单个 epoch 内的循环计算，专注于模型的核心逻辑。

### 指挥官 (`Trainer`) 的工作流程：

1.  **初始化**: 创建模型、优化器、学习率调度器 (`ReduceLROnPlateau`) 和用于混合精度训练的梯度缩放器 (`GradScaler`)。
2.  **加载状态 (Resume)**: 启动时，它会智能地在 `checkpoint_dir` 中查找最新的检查点。如果找到，它会恢复模型权重、优化器状态、当前 epoch 等所有信息。特别地，如果找到的是 `INTERRUPTED.pth.tar`，它会从上次中断的那个批次 (`batch_idx`) 开始，实现无缝续训。
3.  **设置“安全网” (Signal Handling)**: 它会注册系统信号处理器，能捕获用户的 `Ctrl+C` (SIGINT) 或系统关闭命令 (SIGTERM)。一旦捕获，它会立即触发一次紧急状态保存，确保用户的操作不会导致数据丢失。
4.  **驱动循环**: 进入主训练循环，按 epoch 调用 `train_epoch` 和 `validate_epoch` 函数，并根据 `validate_epoch` 的结果（验证集损失）来更新学习率调度器。
5.  **保存状态 (Checkpoint)**: 在每个 epoch 结束后，保存一次常规检查点 (`checkpoint.pth.tar`)。如果当前模型的验证性能超越了历史最佳，还会额外保存一份 `model_best.pth.tar`。

### 执行者 (`train_epoch`) 的核心算法思想：

`train_epoch` 函数内部是训练效率和稳定性的关键所在，它遵循以下步骤：

1.  **自动混合精度 (`autocast`)**: 整个前向传播过程被包裹在 `with autocast(...)` 上下文中。这使得兼容的 GPU 操作（如矩阵乘法）会自动使用半精度 `float16` 计算，极大提升速度并降低显存占用。
2.  **梯度缩放 (Gradient Scaling)**: 由于 `float16` 的数值范围较小，微小的梯度容易下溢变为零。因此，损失在反向传播前会由 `scaler` 乘以一个巨大的缩放因子，将梯度“放大”到一个安全的数值范围。
3.  **梯度累积 (Gradient Accumulation)**: `optimizer.step()` 不再是每个批次都执行，而是累积 `gradient_accumulation_steps` 个批次的梯度后才执行一次。这等效于使用了更大的批次进行训练，是一种在显存有限的情况下提升模型性能的常用技巧。
4.  **梯度反缩放与裁剪 (Unscaling & Clipping)**: 在执行优化前，`scaler` 会将梯度“反缩放”回其原始尺度。随后，通过 `clip_grad_norm_` 对梯度进行裁剪，防止因梯度爆炸导致的训练不稳定。

## 3. 常见问题与解决方案 (FAQ)

- **问：训练中途报错 `CUDA out of memory`，怎么办？**
  - **答：** 这是显存不足的典型错误。首选方案是在 `config.py` 中减小 `batch_size`。如果希望保持大的有效批量，可以相应地增大 `gradient_accumulation_steps` 的值来补偿。

- **问：我不小心用 `Ctrl+C` 中断了训练，还能接着练吗？**
  - **答：** 完全可以。直接重新执行上次的训练命令即可。引擎的信号处理机制已经为您保存了中断时的状态，会自动从断点处恢复。

- **问：我想从头开始一次全新的训练，如何操作？**
  - **答：** 只需删除 `config.py` 中指定的 `checkpoint_dir` 目录，或将其指向一个新目录即可。引擎在启动时找不到任何检查点，就会自动从 `epoch 1` 开始。

- **问：训练日志里，验证集损失 (`Val Loss`) 很久不下降了，正常吗？**
  - **答：** 正常。请观察日志中的学习率 (`learning_rate`)。当 `Val Loss` 在 `patience`（例如3）个 epoch 内没有改善时，`ReduceLROnPlateau` 调度器会自动降低学习率。这是模型在尝试进入更精细的优化阶段。如果学习率已经很低且损失不再变化，说明模型可能已收敛。
